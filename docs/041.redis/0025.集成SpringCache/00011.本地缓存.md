# 本地进程缓存

通常的把缓存分成两种：
1. 分布式缓存: redis, memcache
    - 优点：存储容量，可靠性好，能够在集群间分享
    - 缺点：访问缓存有网络开销
    - 场景：缓存数据量较大、可靠性要求较高、需要在集群间共享
2. 进程本地缓存：HashMap，GuavaCache, Caffeine
    - 优点：读取本地内存，没有网络开销
    - 缺点：存储容量有限，可靠性较低，无法共享
    - 场景：性能要求较高、存储数据较少

## Caffeine使用例子

```java
    Cache<String, String> cache = Caffeine.newBuilder().build();
    cache.put("user", "xx");
    String gf = cache.getIfPrecent("user");
```
### 写入策略
Caffeine有三种缓存写入策略：手动、同步加载、异步加载

### 缓存值的清理策略
Caffeine有三种缓存值的清理策略：基于大小，基于时间和基于引用

#### 1. 基于大小
当缓存大小超过了配置的大小时，会发生回收

#### 2. 基于时间
1. 写入后到期策略
2. 访问后带起策略
3. 到期时间由Expiry实现独自计算

#### 3. 基于引用
1. Java有四种引用：强引用、软引用、弱引用和虚引用，caffeine可以将值封装成弱引用或者软引用
2. 软引用，如果一个对象有软引用，那么会在垃圾回收后内存仍不够的情况下被回收。
3. 弱引用，在下次垃圾回收时，一定会被回收

#### 缓存的清理
缓存的清理并不是立刻执行的，需要有一定的时间，所以应该淘汰的项不一定立马就会被淘汰

### 提供信息统计
Caffeine提供了一种记录缓存使用统计信息的方式，可以实时监控缓存当前的状态，以评估缓存的健康程度以及缓存命中率等，方便后续调整参数。

### 高效的缓存淘汰算法
Caffeine结合了LRU和LFU，建立了自己的缓存淘汰算法-Window TinyLfu

LFU：
1. 它需要额外给每个记录项维护频率信息，每次访问都需要更新，需要一个巨大的空间记录所有出现过的key和其对应的频次
2. 如果数据访问模式随时间有变，LFU的频率信息无法随之变化，因此早先频繁访问的记录都会占据缓存，而后期访问较多的记录而无法被命中。（例如早期某些数据一直被访问，而后期他们不再热点，后期出现新的热点数据而无法使用缓存）

LRU：
1. 实现简单，内存占用低
2. 但是不能反馈真实的频率，如mysql的LRU队列就会出现假热点数据的情况，而将LRU队列分成old区和new区(也称之为热点区)，只有在其中满足一段时间以后才能再加入new区。

#### mysql的LRU策略
1. 把整个LRU队列分为热点区new区和非热点区old区
2. 第一次加载的页会被放到old区的头部，那这样就不会说冲掉了热点区的数据
3. 第二次读取的时候，如果超过了innodb_old_blocks_time的这个设置，就顺利成章的放到了new区的头部
4. 为什么要这么做，就是为了提高缓存的命中率，由于有全表扫描的存在，缓存中容易进入一些被访问但是不是常用的页，却放到了new区的头部。
5. 那么就要区分出真正热点的数据，只有在一定时间内还活在这个缓存链表里面的，那才说明它是热点数据。不热点的早就被干掉了。


## Window TinyLFU

### TinyLFU
原理类似于BloomFilter(布隆过滤器)，在TinyLFU中，把多个bit看成一个整体，来统计一个key的使用频率。TinyLFU也是将key通过多次hash计算来映射到一个个的bit组。在读取的时候，通过读取对应的bit组来知道统计数据。

在Caffeine中维护了一个4-bit CountMinSketch来记录key的使用频率。4-bit也就意味着key的最大使用频率为15。

为了解决数据访问模式变化以及计数的无限增长，TinyLFU使用到了一种基于滑动窗口的时间衰减设计机制，借助于一种简易的reset操作：每次添加一条记录到Sketch的时候，都会给计数器加一，当计数器达到了一个尺寸W的时候，就会把所有的纪律的Sketch数据都除以2，该reset操作可以起到衰减的作用。在这里就是当有一个值达到了15，那么就会把所有的数字都除以2.

### 稀疏突发
新突发产生的key无法建立足够的频率来保留在缓存中，从而导致不断的cache-miss。 所以通过一个window，设计W-TinyLFU来解决这个问题。

### W-TinyLFU
主缓存（main cache）使用SLRU逐出策略和TinyLFU接纳策略，而窗口缓存使用LRU逐出策略而没有任何接纳策略。

主缓存被划分为A1,A2两个区域，80%的空间被分配给热门项目（A2），并且从20%的非热门项目（A1）中挑选victim（淘汰的）。所有的请求的key都会允许进入窗口缓存，而窗口的victim则被允许进入主缓存通过LFU策略去淘汰。

其实就相当于有一个LRU策略的小窗口，为了解决突发的热点请求，同时有一个TinyLFU的大内存来纪录真实的访问情况。
而其中具体就是，两个部分的被淘汰的项也都会经过TinyLFU策略去判断。